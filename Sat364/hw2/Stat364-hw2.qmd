---
title: "Stat364 ~ HW2"
author: "Mert Göksel"
format: pdf
---

```{r, include=FALSE}
setwd("D:/Stat/R/Stat364/hw2")
library(caret)
library(VGAM)
library(knitr)
library(AER)
library(openxlsx)
library(MASS)
library(data.table)
library(GGally)
library(ggpubr)
library(tidyverse)
library(reshape2)
options(scipen=999)
```

# Q1

```{r}
df <- read.xlsx("./accidents.xlsx")
df
```
This is pandas multiindex thus need reset_index().

```{r}
library(reticulate)
use_python("C:\\Python\\envs\\myenv\\Scripts\\python.exe")
```

```{python}
import pandas as pd
df = pd.read_excel("./accidents.xlsx", index_col=[0,1,2]).reset_index()
df.head()
```
```{r}
df <- py$df
head(df) 
```

Our conversion to r has been completed. Now we can begin with the analysis. 

First, I want to see the visualizations in order to come up with an analysis 
plan. 

```{r}
df <- df %>% rename(y1 = "1", 
                    y2 = "2", 
                    y3 = "3", 
                    y4 = "4", 
                    y5 = "5", Seat = `Seat-Belt`)
df %>% reshape2::melt() %>% 
  ggplot(aes(x=variable, y=value, fill=Seat)) +
  geom_bar(stat="identity",position=position_dodge(),color='black') +
  facet_wrap(vars(Gender, Location))
```

We know that 1st variable is the "not injured" variable and we see from 
the barplot that males have an approximately equal amount of non injured 
with seatbelt on and off. Other than that area type doesnt seem to affect 
the results of the crash. 

now lets build our model

```{r}
model <- vglm(cbind(y1,y2,y3,y4,y5)~Seat+Location+Gender, 
              data=df, family = cumulative(parallel=TRUE))
summary(model)
```
Our model is;

$$
\tiny
\begin{aligned} 
 logit(P(Y \leq Not\; injured))=1.201115-0.825203.seatbelt+0.775308.locationisurban+0.545428.Genderismale \\
 logit(P(Y \leq Injured\; faintly\;-nottransported))=1.376190-0.825203.seatbelt+0.775308.locationisurban+0.545428.Genderismale \\
 logit(P(Y \leq Injured\; faintly\;- transported))=3.242534-0.825203.seatbelt+0.775308.locationisurban+0.545428.Genderismale \\
 logit(P(Y \leq Injured\; heavily\;))=5.150100-0.825203.seatbelt+0.775308.locationisurban+0.545428.Genderismale
\end{aligned}
$$

For any fixed j, the estimated odds that a drivers injury where the crash 
happened in an urban area is in the better side rather than the worse 
direction (i.e., Y ≤ j rather than Y > j) equal exp(β_2)=exp(0.775308)=2.171261 
times the estimated odds for rural.

```{r, message=FALSE}
kable(data.frame(gender=df$Gender, location=df$Location, 
                 seatbelt = df$Seat, prob=fitted(model)))
```

## Q2

```{r}
cereal <- read.csv("./cereal_dillons.csv")
cereal %>% count(Cereal) %>% nrow() #38 different cereals
``` 

lets re-format our data:

```{r}
stand01 <- function(x){(x-min(x))/(max(x)-min(x))}
cereal2 <- data.frame(Shelf=cereal$Shelf,
                      sugar=stand01(x=cereal$sugar_g/cereal$size_g),
                      fat=stand01(x=cereal$fat_g/cereal$size_g),
                      sodium=stand01(x=cereal$sodium_mg/cereal$size_g))
head(cereal2) #much better
```

## Part A

```{r}
#Estimate a multinomial regression model with linear forms of the sugar, 
#fat, and sodium variables. 

model <- vglm(Shelf~., data=cereal2, family = multinomial)
summary(model)
```

```{r}
#perform lrt to each variable

#for sugar
VGAM::lrtest(model, vglm(Shelf~fat+sodium, cereal2, family = multinomial))
```
Sugar is significant

```{r}
#for fat
VGAM::lrtest(model, vglm(Shelf~.-fat, cereal2, family = multinomial))
```
Fat seems to be not significant 

```{r}
#for sodium
VGAM::lrtest(model, vglm(Shelf~.-sodium, cereal2, family = multinomial))
```
And sodium is significant.

## Part B

```{r}
stand02 <- \(x, y, z){ 
  z <- (cereal %>% select(z))/(cereal %>% select(size_g))
  (x/y - min(z))/(max(z)-min(z))
}

pred <- data.frame(sugar = stand02(12,28, "sugar_g"),
                   fat = stand02(0.5, 28, "fat_g"),
                   sodium = stand02(130, 28, "sodium_mg"))

prob <- predict(model, pred, type = "response", se.fit = F)
prob
```

Most probable shelf is the second.

## Part C

```{r}
data_prob <- data.frame(Shelf = cereal$Shelf, 
                        sugar = stand01(x = cereal$sugar_g/cereal$size_g),
                        fat = mean(stand01( x = cereal$fat_g/cereal$size_g)),
                        sodium = mean(stand01(x = cereal$sodium_mg/cereal$size_g )))

probs <- as.data.frame(predict(model, data_prob, type = "response", se.fit = F))
colnames(probs) <- c("y1","y2","y3","y4")

probs %>% mutate(sugar = data_prob$sugar) %>% 
  arrange(sugar) %>% tidyr::gather("id", "value", 1:4) %>%
  ggplot(aes(x=sugar, y=value, color=id)) + geom_line()
```


## Part D

```{r}
int <- confint(model, level = .95)
round(exp(int), 4)
```

# Q3

```{r}
df <- read.xlsx("heart.xlsx")
kable(head(df))
```

## Part A

```{r}
model <- glm(visits ~., df, family = poisson(link = "log"))
summary(model)
```

The response function is:

$$
\begin{aligned}
ln(Y) = &e^{0.4784+id*0.0001+totalcost*0+age*0.0067+gender*0.1809+interventions*0.0101} *\\
&e^{drugs*0.1934+complications*0.062+comorbidities*-0.0009+duration*0.0003}
\end{aligned}
$$

## Part B

```{r}
best.model <- step(model, direction = "backward", trace = F)$call
best.model <- glm(best.model, family = poisson(link = "log"), data = df)
summary(best.model)
```

## Part C

```{r}
deviance(model) > qchisq(0.05, df.residual(model), lower.tail = F)
```
Since test statistic is more extreme this is not a good fit.

## Part D

```{r}
dispersiontest(model)
```

Since p value less than 0.05 we can say that there is overdispersion.

```{r}
model.new1 <- glm(visits~., family = quasipoisson, df)
summary(model.new1)

model.new2 <- glm.nb(visits~., df)
summary(model.new2)

data.frame(quassi = 
             summary(model.new1)$deviance/summary(model.new1)$df.residual,
           negbinom = 
             summary(model.new2)$deviance/summary(model.new2)$df.residual) %>%
  mutate(whichbetter = ifelse(quassi > negbinom, "negbinom", "quassi"))

```

Model fits better with negative binomial.

# Q4

```{r}
df <- read.xlsx("mcgill.xlsx")
kable(head(df))
```

## Part A

```{r}
model <- lm(y~x, df)
residuals <- model$residuals

plot(df$y, residuals)
```

I think there is some seasonal trend almost like $sin(x)$ like.

## Part B

```{r}
durbinWatsonTest(model, alternative = "positive")
```

Since p value is 0 < 0.01 we reject the null hypothesis so there is
positive autocorrelation.

## Part C

```{r}
res <- model$residuals
sum(res[2:20]*res[1:19])/sum(res[1:19]^2)
```

## Part D

```{r}
yt <- df$y[2:20] - 0.6729603*df$y[1:19]
xt <- df$y[2:20] - 0.6729603*df$x[1:19]

transformed_model <- lm(yt~xt)
summary(transformed_model)
```

## Part E

```{r}
durbinWatsonTest(transformed_model, alternative = "positive")
```

Since p value is now above 0.05 there is no autocorrelation.

# Q5

If we have influential/leverage points then we need to weigth each 
observation where higher the residual lesser the weight. In order 
to do this we first select a weight function. I will select the 
bisquare function for example where;

$$
\begin{cases} 
  [1-(\frac{u}{4.685})^2]^2 &, |u| \leq 4.685 \\
  0 &, |u| > 4.685  
\end{cases}
$$

```{r}
bisq <- \(x) (1-(x/4.685)^2)^2 #func
u = seq(-4.685,4.685,by=.1) #range
plot(u,bisq(u),type='l',ylab='Weight Function') #plot
```
Values of u near 0 has bigger weights and values farther away has less and less. 

Then we initialize the weights. If all weights are same then its the same 
idea with ordinary least squares 

3rdly we do the weighted least squares with the weights we just initialized and
get the fitted model. The formula for coefficients then will be 

$$
b_w = (X'WX)^{-1} X'Wy
$$

we use this initial model to get an initial set of residuals. For each residual
we scale it via the formula 

$$
u_i = \frac{e_i}{MAD}
$$

where $MAD$ is the Median Absolute Deviation. Which is;

$$
MAD = \frac{1}{.6745}median(|e_i - median(e_i)|) 
$$

- $\frac{1}{.6745}$ is the coefficient which makes mad unbaised. 

then we plug our $u_i$ vector into bisq function again to obtain weight vector.

we repeat the last 2 process many times until it starts to stabilize. 



















