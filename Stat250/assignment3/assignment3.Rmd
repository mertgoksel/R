---
title: "Q6"
author:
  - "Mert Göksel"
  - "Bilge Özkır"
  - "Aisuluu Baktybekova"
output:
  pdf_document: default
  html_notebook: default
---

```{r}
library(ggpubr)
library(rstatix)
library(tidyverse)
library(reshape2)
library(broom)
```

## Data Prep:
```{r}
ap_1 <- c(1000,
          1500,
          1200,
          1800,
          1600,
          1100,
          1000,
          1250)
ap_2 <- c(1500,
          1800,
          2000,
          1200,
          2000,
          1700,
          1800,
          1900)
ap_3 <- c(900,
          1000,
          1200,
          1500,
          1200,
          1550,
          1000,
          1100)
df <- cbind(ap_1, ap_2, ap_3)
```

## A:
```{r}
#We will use ANOVA to test to see if the observations 
#suggest a difference between results of methods.
#H0: Pop Means are equal for all 3 groups
#H1: Pop Means are not equal for at least one group

#To apply ANOVA we need to see first if these samples can be assumed to be normal
apply(df, 2, shapiro.test)
#All of them seems to be good to use for ANOVA

#Lets see if they have any outliers
par(mfrow = c(1,3))
boxplot(df[,1])
boxplot(df[,2])
boxplot(df[,3])
#None has outliers

#Lets check homogenity of variances
melt(df) %>% levene_test(formula = value~Var2)
#P value is higher than 0.05 thus there is no evidence for heterogenity of variance

#Finally we can use anova
model <- aov(formula = value~Var2, data = melt(df))
model %>% summary()
#There are significant differences between groups as p value < 0.05
#But we do not know which of these 2,3 groups are different, 
#this p value only tells us that there is a group that
#is different. So we can use Tukey comparison,or do pairwise ttest to each pair
#to see which groups are different from each other.
```

## B:
```{r}
summary(resid(model))
#We see that mean is 0.

par(mfrow = c(2,2))
plot(model)

#First plot is showing how the residuals behave, we see no trend nor difference 
#from zero. 

#But there are some outliers namely 4, 12, 22.
#This may result in heterogeneity of variances or non normality.
#So we test for these. 

#We already applied levenes test to see 
#if variances are homogeneous and gotten good results.
#So lets test for normality of residuals. Looking at the qqplot 
#it seems this test will be satisfactory, but lets test it anyways.
shapiro.test(resid(model))
#0.9 p value which is absolutely assumable to be normal. 

#Now model adequacy is shown by Rsquared. Rsquared is from linear model.
#Linear model, if applied the same formula (value~Var2), will yield the same result.
#Lets test.
summary(model)
anova(lm(value~Var2, data = melt(df)))
#See? Now we can use this regression model to get Rsquared

summary(lm(value~Var2, data = melt(df)))
#R^2 = 47.26%. Meaning only 47.26% of total variation is explained by anova model.
#As this is oneway anova we dont need to consider adjusted rsquared.

#We can also find this R^2 with its formula.
tidy_aov <- tidy(model)
tidy_aov
sum_squares_regression <- tidy_aov$sumsq[1]
sum_squares_residuals <- tidy_aov$sumsq[2]
Rs <- sum_squares_regression/(sum_squares_regression+sum_squares_residuals)
Rs
```


